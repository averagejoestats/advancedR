
---
title: ""
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Statistical Computing - Exercises 07 - Performance

We return to two problems and add a third: calculating song statistics in the
billboard dataset, calculating distances in the flights dataset, and
reformatting a hurricane track dataset called hurdat. Below are correct
solutions to all three problems. Your task is to make them run faster, while
ensuring that your faster code gives the correct result.

Show how long your revised code takes to run, and demonstrate that
it gives the right answer.

## Song Statistics

```{r}
hot <- read.csv("../datasets/Hot_100.csv")

weeks_below_k <- function( dat, k ){

    # computes the number of weeks on chart at or below ranking k
    # for each songid in dat

    # get the vector of unique song ids
    unique_songs <- unique( dat$song_id )

    # initialize vector for number of weeks at or below ranking k
    weeks_below_k <- c()

    # loop over all the songs
    for(j in 1:length( unique_songs ) ){

        # find the indices for this song
        inds_song <- which( dat$song_id == unique_songs[j] )
         
        # initialize a count of number of weeks at or below k
        count <- 0

        # loop over the indices
        for( i in inds_song ){ 

            # add to the count if the week_position is at or below k
            if( dat$chart_position[i] <= k ){
                count <- count + 1
            }

        }

        # update weeks_below_k with this song's count
        weeks_below_k <- c( weeks_below_k, count )

    }
    # rank the songs
    ord <- order( weeks_below_k, decreasing = TRUE )
    return( data.frame( song_id = unique_songs[ord], weeks_below_k = weeks_below_k[ord] ) )
}

r <- weeks_below_k( hot, 1 )
head(r, n = 20 )
```


## Distance matrix for airline data

```{r}
# distance matrix
dat <- read.csv("../datasets/airline_2019-07-01.csv")

get_distance_matrix <- function( dat ){

    # get the set of unique airports
    ports <- sort( unique( c(dat$Origin, dat$Dest) ) )
    nports <- length(ports)
    
    # set up a distance matrix
    distmat <- matrix(NA, nports, nports )
    rownames(distmat) <- ports
    colnames(distmat) <- ports
    
    # loop over all possible origins and destinations
    for(p1 in ports){
        for(p2 in ports){
    
            # get row and column indices for this pair
            j1 <- which( rownames(distmat) == p1 )
            j2 <- which( colnames(distmat) == p2 )
    
            # get rows of data frame for this pair, and subset
            ii <- which( dat$Origin == p1 & dat$Dest == p2 ) 
            subdat <- dat[ii,]
    
            # find the distance
            distmat[j1,j2] <- subdat$Distance[1]
    
        }
    }
    return(distmat)
}

distmat <- get_distance_matrix( dat )
v <- c("ATL","ORD","LGA","JFK","DEN","LAX","STL","SEA")
distmat[v,v]
```

## Hurricane data

```{r}
# read in the raw data
dat <- read.csv("../datasets/hurdat2-atl-02052024.txt", header= FALSE)

# initialize the processed dataset
hurdat <- data.frame( matrix(NA, 0, ncol(dat)+1) )
colnames(hurdat) <- c("hur_code", colnames(dat))

# counter for the row of hurdat
k <- 0

# loop over rows of raw dataset
for(j in 1:nrow(dat)){

    # extract the current row of raw data
    this_row <- dat[j,]

    # check whether this is a code row
    if( substr( this_row[1,1], 1, 2 ) == "AL" ){

        # if so, update the hurricane code
        hur_code <- this_row[1,1]

    } else {

        # otherwise update the counter and write to the next row
        k <- k + 1
        hurdat[k,] <- cbind( hur_code, this_row )

    }
}
```

    
